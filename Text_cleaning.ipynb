{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #realestate In Atlanta, Housing Woes Reflect N...\n",
       "1    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "2    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "3    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "4    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re \n",
    "import string\n",
    "df=pd.read_csv(r'C:\\Users\\myloe\\OneDrive\\Desktop\\combined.csv')\n",
    "df.text.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #realestate In Atlanta, Housing Woes Reflect N...\n",
       "1    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "2    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "3    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "4    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing URls \n",
    "import re\n",
    "def url_remove(textur):\n",
    "    textunur=re.sub(r\"\\S*https?:\\S*\", \"\", textur)\n",
    "    return textunur\n",
    "df.text = df.text.apply(lambda x: url_remove(x))\n",
    "df.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Fixing Contractions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcontractions\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfix_contract\u001b[39m(textcont):\n\u001b[0;32m      4\u001b[0m     textfix\u001b[39m=\u001b[39mcontractions\u001b[39m.\u001b[39mfix(textcont)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "#Fixing Contractions\n",
    "import contractions\n",
    "def fix_contract(textcont):\n",
    "    textfix=contractions.fix(textcont)\n",
    "    return textfix\n",
    "df.text=df.text.apply(lambda x: fix_contract(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    realestate In Atlanta, Housing Woes Reflect Na...\n",
       "1    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "2    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "3    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "4    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "import re\n",
    "def hash_remove(texthash):\n",
    "    textunhash=re.sub(r'#', '', texthash)\n",
    "    return textunhash\n",
    "df.text = df.text.apply(lambda x: hash_remove(x))\n",
    "df.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary consisting of apostrphe's\n",
    "import re\n",
    "def s_remove(texts):\n",
    "    textuns=re.sub(r\"'s\", '', texts)\n",
    "    return textuns\n",
    "df.text = df.text.apply(lambda x: s_remove(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary consisting of apostrphe’s is\n",
    "\n",
    "import re\n",
    "def is_fix(textunis):\n",
    "    textis=re.sub(r\"’s\", '', textunis)\n",
    "    return textis\n",
    "df.text = df.text.apply(lambda x: is_fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punctuation\n",
    "\n",
    "import re\n",
    "def punc_fix(textpun):\n",
    "    textunpun=re.sub(r'[^\\w\\s]', '', textpun)\n",
    "    return textunpun\n",
    "df.text = df.text.apply(lambda x: punc_fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all of the digits in the string with an empty string.\n",
    "import re\n",
    "def num_fix(textnum):\n",
    "    textnonum=re.sub(r'[0-9]', '', textnum)\n",
    "    return textnonum\n",
    "df.text = df.text.apply(lambda x: num_fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code to remove whitespace\n",
    "def removewh(textwh):\n",
    "    return \" \".join(textwh.split())\n",
    "df.text = df.text.apply(lambda x: removewh(x))\n",
    "\n",
    "\n",
    "# Python3 code to lowercase\n",
    "def lowercase(textU):\n",
    "    return textU.lower()\n",
    "df.text = df.text.apply(lambda x: lowercase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\myloe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\myloe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Removing Stop Words\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Bring in the default English NLTK stop words\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Additional stop words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Open a file and read it into memory\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mAdditional stop words.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m texti \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Additional stop words.txt'"
     ]
    }
   ],
   "source": [
    "# Open a file and read it into memory\n",
    "file = open('Additional stop words.txt')\n",
    "texti = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the stoplist to the text\n",
    "additional_stopwords = [word for word in texti.split() if word not in stoplist]\n",
    "stoplist += additional_stopwords\n",
    "\n",
    "for i in range(len(stoplist)):\n",
    "    stoplist[i] = stoplist[i].lower()\n",
    "\n",
    "print (stoplist)\n",
    "\n",
    "def token(textUnT):\n",
    "    text_tokens = word_tokenize(textUnT)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stoplist]\n",
    "    return tokens_without_sw\n",
    "df.text = df.text.apply(lambda x: token(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 0         [#, r, e, a, l, e, s, t, a, t, e,  , I, n,  , ...\n",
       "1         [I, n,  , A, t, l, a, n, t, a, ,,  , H, o, u, ...\n",
       "2         [I, n,  , A, t, l, a, n, t, a, ,,  , H, o, u, ...\n",
       "3         [I, n,  , A, t, l, a, n, t, a, ,,  , H, o, u, ...\n",
       "4         [I, n,  , A, t, l, a, n, t, a, ,,  , H, o, u, ...\n",
       "                                ...                        \n",
       "359908    [W, o, m, a, n,  , a, n, d,  , c, h, i, l, d, ...\n",
       "359909    [D, i, d,  , y, o, u,  , a, t, t, e, n, d,  , ...\n",
       "359910    [\", M, y,  , l, i, f, e,  , f, e, l, t,  , l, ...\n",
       "359911    [O, n, e,  , e, f, f, i, c, i, e, n, t,  , w, ...\n",
       "359912    [\", C, o, n, v, e, r, s, i, o, n, s,  , a, r, ...\n",
       "Name: text, Length: 359913, dtype: object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "df.text = df.text.apply(lambda x: [lm.lemmatize(y) for y in x])\n",
    "df.text.head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c75de23fdfb89a8eccdb32f776797e080ba5528a78d35795fb0867c8517716f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
