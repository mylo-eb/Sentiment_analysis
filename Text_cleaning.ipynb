{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #realestate In Atlanta, Housing Woes Reflect N...\n",
       "1    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "2    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "3    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "4    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re \n",
    "import string\n",
    "df=pd.read_csv(r'C:\\Users\\myloe\\OneDrive\\Desktop\\combined.csv')\n",
    "df.text.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #realestate In Atlanta, Housing Woes Reflect N...\n",
       "1    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "2    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "3    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "4    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing URls \n",
    "import re\n",
    "def url_remove(textur):\n",
    "    textunur=re.sub(r\"\\S*https?:\\S*\", \"\", textur)\n",
    "    return textunur\n",
    "df.text = df.text.apply(lambda x: url_remove(x))\n",
    "df.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing Contractions\n",
    "#import contractions\n",
    "#def fix_contract(textcont):\n",
    "    #textfix=contractions.fix(textcont)\n",
    "    #return textfix\n",
    "#df.text=df.text.apply(lambda x: fix_contract(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    realestate In Atlanta, Housing Woes Reflect Na...\n",
       "1    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "2    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "3    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "4    In Atlanta, Housing Woes Reflect Nation’s Econ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "import re\n",
    "def hash_remove(texthash):\n",
    "    textunhash=re.sub(r'#', '', texthash)\n",
    "    return textunhash\n",
    "df.text = df.text.apply(lambda x: hash_remove(x))\n",
    "df.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary consisting of apostrphe's\n",
    "import re\n",
    "def s_remove(texts):\n",
    "    textuns=re.sub(r\"'s\", '', texts)\n",
    "    return textuns\n",
    "df.text = df.text.apply(lambda x: s_remove(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary consisting of apostrphe’s is\n",
    "\n",
    "import re\n",
    "def is_fix(textunis):\n",
    "    textis=re.sub(r\"’s\", '', textunis)\n",
    "    return textis\n",
    "df.text = df.text.apply(lambda x: is_fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punctuation\n",
    "\n",
    "import re\n",
    "def punc_fix(textpun):\n",
    "    textunpun=re.sub(r'[^\\w\\s]', '', textpun)\n",
    "    return textunpun\n",
    "df.text = df.text.apply(lambda x: punc_fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all of the digits in the string with an empty string.\n",
    "import re\n",
    "def num_fix(textnum):\n",
    "    textnonum=re.sub(r'[0-9]', '', textnum)\n",
    "    return textnonum\n",
    "df.text = df.text.apply(lambda x: num_fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code to remove whitespace\n",
    "def removewh(textwh):\n",
    "    return \" \".join(textwh.split())\n",
    "df.text = df.text.apply(lambda x: removewh(x))\n",
    "\n",
    "\n",
    "# Python3 code to lowercase\n",
    "def lowercase(textU):\n",
    "    return textU.lower()\n",
    "df.text = df.text.apply(lambda x: lowercase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\myloe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\myloe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Removing Stop Words\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Bring in the default English NLTK stop words\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open a file and read it into memory\n",
    "file = open('Additional stop words.txt')\n",
    "texti = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the stoplist to the text\n",
    "additional_stopwords = [word for word in texti.split() if word not in stoplist]\n",
    "stoplist += additional_stopwords\n",
    "\n",
    "for i in range(len(stoplist)):\n",
    "    stoplist[i] = stoplist[i].lower()\n",
    "\n",
    "print (stoplist)\n",
    "\n",
    "def token(textUnT):\n",
    "    text_tokens = word_tokenize(textUnT)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stoplist]\n",
    "    return tokens_without_sw\n",
    "df.text = df.text.apply(lambda x: token(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 0         [r, e, a, l, e, s, t, a, t, e,  , i, n,  , a, ...\n",
       "1         [i, n,  , a, t, l, a, n, t, a,  , h, o, u, s, ...\n",
       "2         [i, n,  , a, t, l, a, n, t, a,  , h, o, u, s, ...\n",
       "3         [i, n,  , a, t, l, a, n, t, a,  , h, o, u, s, ...\n",
       "4         [i, n,  , a, t, l, a, n, t, a,  , h, o, u, s, ...\n",
       "                                ...                        \n",
       "359908    [w, o, m, a, n,  , a, n, d,  , c, h, i, l, d, ...\n",
       "359909    [d, i, d,  , y, o, u,  , a, t, t, e, n, d,  , ...\n",
       "359910    [m, y,  , l, i, f, e,  , f, e, l, t,  , l, i, ...\n",
       "359911    [o, n, e,  , e, f, f, i, c, i, e, n, t,  , w, ...\n",
       "359912    [c, o, n, v, e, r, s, i, o, n, s,  , a, r, e, ...\n",
       "Name: text, Length: 359913, dtype: object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "df.text = df.text.apply(lambda x: [lm.lemmatize(y) for y in x])\n",
    "df.text.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\myloe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\myloe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filtered_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m     tokens_without_sw \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word \u001b[39min\u001b[39;00m stoplist]\n\u001b[0;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m tokens_without_sw\n\u001b[1;32m---> 31\u001b[0m filtered_df\u001b[39m.\u001b[39mtext \u001b[39m=\u001b[39m filtered_df\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: token(x))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Tokenize, remove stop words, and non-essential POS\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')  # for pos_tag function\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag  # for pos_tag function\n",
    "print(stopwords.words('english'))\n",
    "# Bring in the default English NLTK stop words\n",
    "stoplist = stopwords.words('english')\n",
    "# Open a file and read it into memory\n",
    "#file = open('Additional stop words.txt')\n",
    "#texti = file.read()\n",
    "# Apply the stoplist to the text\n",
    "#additional_stopwords = [word for word in texti.split() if word not in stoplist]\n",
    "#stoplist += additional_stopwords\n",
    "stoplist = [word.lower() for word in stoplist]\n",
    "#additional_stopwords = [word.lower() for word in additional_stopwords]\n",
    "for i in range(len(stoplist)):\n",
    "    stoplist[i] = stoplist[i].lower()\n",
    "print(stoplist)\n",
    "def token(textUnT):\n",
    "    text_tokens = word_tokenize(textUnT)\n",
    "    # Tag the parts of speech of the tokens\n",
    "    pos_tags = pos_tag(text_tokens)\n",
    "    # Keep only the words that are verbs, nouns, adjectives, or adverbs\n",
    "    tokens = [word for word, tag in pos_tags if tag in ['NN','NNS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP']]\n",
    "    # Remove stop words and return the filtered list of tokens\n",
    "    tokens_without_sw = [word for word in tokens if not word in stoplist]\n",
    "    return tokens_without_sw\n",
    "filtered_df.text = filtered_df.text.apply(lambda x: token(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwordcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m WordCloud\n\u001b[0;32m      3\u001b[0m \u001b[39m# Flatten the list of tokens into a single list\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m all_tokens \u001b[39m=\u001b[39m filtered_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msum()\n\u001b[0;32m      5\u001b[0m \u001b[39m# Join the list of tokens into a single string\u001b[39;00m\n\u001b[0;32m      6\u001b[0m all_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(all_tokens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_df' is not defined"
     ]
    }
   ],
   "source": [
    "#wordCloud\n",
    "from wordcloud import WordCloud\n",
    "# Flatten the list of tokens into a single list\n",
    "all_tokens = filtered_df['text'].sum()\n",
    "# Join the list of tokens into a single string\n",
    "all_text = ' '.join(all_tokens)\n",
    "# Generate a wordcloud from the string\n",
    "wordcloud = WordCloud(max_font_size=30, background_color='white').generate(all_text)\n",
    "#, contour_color = 'black', contour_width = 2, color_func = lambda *args, **kwargs: 'black'\n",
    "# Set the width and height of the figure\n",
    "plt.figure(figsize=(80, 50))\n",
    "# Display the wordcloud\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# Turn off the axis labels\n",
    "plt.axis(\"off\")\n",
    "# Set the title of the plot\n",
    "plt.title(\"Word Cloud of Tweets\")\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m FreqDist\n\u001b[0;32m      4\u001b[0m \u001b[39m# Tokenize the text\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(all_text)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Count the frequency of each word\u001b[39;00m\n\u001b[0;32m      7\u001b[0m fdist \u001b[39m=\u001b[39m FreqDist(tokens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_text' is not defined"
     ]
    }
   ],
   "source": [
    "#get the top frequent words\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(all_text)\n",
    "# Count the frequency of each word\n",
    "fdist = FreqDist(tokens)\n",
    "# Convert the FreqDist object to a dataframe\n",
    "import pandas as pd\n",
    "word_freq_df = pd.DataFrame(fdist.items(), columns=['word', 'frequency'])\n",
    "# Sort the dataframe by frequency\n",
    "word_freq_df = word_freq_df.sort_values(by='frequency', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c75de23fdfb89a8eccdb32f776797e080ba5528a78d35795fb0867c8517716f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
